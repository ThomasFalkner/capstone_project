{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bundesanzeiger Scraping\n",
    "\n",
    "Der folgende Code scraped die Webseite https://www.bundesanzeiger.de/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for html network requests and parsing:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "# for displaying the captcha images:\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "# for file importing and exporting:\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "# for waiting: \n",
    "from datetime import datetime\n",
    "import time\n",
    "# for audio:\n",
    "import simpleaudio as sa\n",
    "# other:\n",
    "import re # regex\n",
    "from anticaptchaofficial.imagecaptcha import *\n",
    "#from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globale Variablen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_requests = requests.session()\n",
    "session_id = \"\"\n",
    "debug_prints = False\n",
    "\n",
    "try:\n",
    "    with open('credentials.json') as json_file:\n",
    "        credentials = json.load(json_file)\n",
    "except Exception as e:\n",
    "    print(\"couldn't read JSON file!\", repr(e))\n",
    "    pass\n",
    "\n",
    "solver = imagecaptcha()\n",
    "solver.set_verbose(1)\n",
    "solver.set_key(credentials['secret_key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basisfunktionen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_search_results\n",
    "\n",
    "Diese Funktion stellt eine Anfrage an die Suchmaske von https://www.bundesanzeiger.de/ und gibt die Ergebnistabelle als Dataframe zurück.\n",
    "\n",
    "Diese Funktion wird direkt aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_search_results(search_string):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "    # genericsearch_param.part_id: 22 is for just Jahresabschlüsse\n",
    "    payload = {'page.navid': 'to_detailsearch', 'global_data.designmode': 'eb', '(page.navid=to_quicksearchlist)': 'Suchen', 'genericsearch_param.part_id': '22', 'genericsearch_param.hitsperpage': '20'}\n",
    "    payload.update({'genericsearch_param.fulltext': search_string})\n",
    "    if session_id != \"\":\n",
    "        payload.update({'session.sessionid': session_id})\n",
    "\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/wexsservlet'\n",
    "    # time.sleep(0.33) # wait some seconds before the next request, to not overwhelm the server\n",
    "    try:\n",
    "        result = session_requests.get(url, params = payload)\n",
    "    except Exception as e:\n",
    "        print('Error in getting search result!', repr(e))\n",
    "        result = None\n",
    "    if debug_prints:\n",
    "        print(\"get_searchresults url:\", result.url)\n",
    "        print(\"get_searchresults returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "    if result is not None:\n",
    "        # Get html content\n",
    "        soup = BeautifulSoup(result.text, \"lxml\")\n",
    "        # Parse for session id, update variable\n",
    "        session_id = soup.find('a', href=True)['href']\n",
    "        session_id = session_id[session_id.find(\"session.sessionid=\")+len(\"session.sessionid=\"):session_id.find(\"&\", session_id.find(\"session.sessionid=\"))]\n",
    "        if debug_prints:\n",
    "            print(\"session_id:\", session_id)\n",
    "\n",
    "        # Parse html content\n",
    "        result_table = soup.find(\"table\", attrs={\"class\": \"result\", \"summary\": \"Trefferliste\"})\n",
    "        rows = result_table.find_all(\"tr\")\n",
    "        table_contents = []\n",
    "        for tr in rows:\n",
    "            # header row\n",
    "            if rows.index(tr) == 0 :\n",
    "                row_cells = [ th.getText(separator=\" \").strip() for th in tr.find_all('th') if th.getText().strip() != '' ] + [\"session.sessionid=\", \"fts_search_list.selected\", \"fts_search_list.destHistoryId\", \"timestamp\"]\n",
    "            # data rows\n",
    "            else:\n",
    "                # get 'th' element text\n",
    "                row_cells = ([ tr.find('th').getText(separator=\" \") ] if tr.find('th') else [] ) + \\\n",
    "                [ td.getText(separator=\" \").strip() for td in tr.find_all('td') if td.getText().strip() != '' ] + \\\n",
    "                [ a['href'][a['href'].find(\"session.sessionid=\")+len(\"session.sessionid=\"):a['href'].find(\"&\", a['href'].find(\"session.sessionid=\"))] for a in tr.find_all('a', href=True)] + \\\n",
    "                [ a['href'][a['href'].find(\"fts_search_list.selected=\")+len(\"fts_search_list.selected=\"):a['href'].find(\"&\", a['href'].find(\"fts_search_list.selected=\"))] for a in tr.find_all('a', href=True)] + \\\n",
    "                [ a['href'][a['href'].find(\"fts_search_list.destHistoryId=\")+len(\"fts_search_list.destHistoryId=\"):] for a in tr.find_all('a', href=True)] + \\\n",
    "                [ time.ctime() ]\n",
    "            if len(row_cells) > 1 :\n",
    "                table_contents += [ row_cells ]\n",
    "\n",
    "        # Convert to dataframe and set first row as headers\n",
    "        df = pd.DataFrame.from_dict(table_contents)\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df[1:]\n",
    "        return df\n",
    "    else:\n",
    "        print('get_search_results had an erorr!')\n",
    "        df = pd.DataFrame()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_captcha\n",
    "\n",
    "Bevor eine Dokumentenanfrage mit dem gewünschen Ergebnis beantwortet wird, muss der User ein Captcha lösen. Hier wird die Lösung dem Nutzer gezeigt und seine Eingabe verlangt.\n",
    "\n",
    "Es gibt hier zwei Funktionen zur Auswahl: get_image_captcha sowie get_image_and_audio_captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gets just the image captcha\n",
    "def get_image_captcha(session_id, save_captcha_files):\n",
    "    global debug_prints\n",
    "    payload = {'state.action':'captcha','captcha_data.mode':'image'}\n",
    "    payload.update({'session.sessionid': session_id})\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/contentloader'\n",
    "    try:\n",
    "        result = session_requests.get(url, params = payload)\n",
    "    except Exception as e:\n",
    "        print('Error in getting image captcha!', repr(e))\n",
    "        result = None\n",
    "    if debug_prints:\n",
    "        print(\"get_captcha url\", result.url)\n",
    "        print(\"get_captcha returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "    if result is not None:\n",
    "        if result.headers['content-type'] == \"image/jpeg\":\n",
    "\n",
    "            ## here the image could be saved as a file:\n",
    "            if save_captcha_files:\n",
    "                try:\n",
    "                    with open('scraped_data/captchas/captcha.jpg', 'w+b') as file:\n",
    "                        file.write(result.content)\n",
    "                except Exception as e:\n",
    "                    print(\"saving captcha.jpg failed!\", repr(e))\n",
    "                    pass\n",
    "\n",
    "            img = Image.open(BytesIO(result.content))\n",
    "            display(img)\n",
    "            print('Please solve this captcha. To cancel, type \"exit\"')\n",
    "            captcha_solution = \"\"\n",
    "            time.sleep(0.1)\n",
    "            # captcha_solution = input()\n",
    "\n",
    "            captcha_solution = solver.solve_and_return_solution('scraped_data/captchas/captcha.jpg')\n",
    "            if captcha_solution != 0:\n",
    "                print(\"captcha text \"+captcha_solution)\n",
    "                return captcha_solution\n",
    "            else:\n",
    "                print(\"task finished with error \"+solver.error_code)\n",
    "                return ''\n",
    "        else:\n",
    "            print(\"no image returned\")\n",
    "            return \"\"\n",
    "    else:\n",
    "        print('get_image_captcha returned an error!')\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# first get a image and then audio captcha, else service doesn't correctly process request\n",
    "def get_image_and_audio_captcha(session_id, save_captcha_files):\n",
    "    global debug_prints\n",
    "    payload = {'state.action':'captcha','captcha_data.mode':'mixed-image'}\n",
    "    payload.update({'session.sessionid': session_id})\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/contentloader'\n",
    "    # here the captcha image is requested\n",
    "    try:\n",
    "        result = session_requests.get(url, params = payload)\n",
    "    except Exception as e:\n",
    "        print('Error in getting image captcha!', repr(e))\n",
    "        result = None\n",
    "    if debug_prints:\n",
    "        print(\"get_captcha url\", result.url)\n",
    "        print(\"get_captcha returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "    if result is not None:\n",
    "        if result.headers['content-type'] == \"image/jpeg\":\n",
    "\n",
    "            # here the image could be saved as a file:\n",
    "            if save_captcha_files:\n",
    "                try:\n",
    "                    with open('scraped_data/captchas/captcha.jpg', 'w+b') as file:\n",
    "                        file.write(result.content)\n",
    "                except Exception as e:\n",
    "                    print(\"saving captcha.jpg failed!\", repr(e))\n",
    "                    pass\n",
    "\n",
    "            img = Image.open(BytesIO(result.content))\n",
    "            display(img)\n",
    "            # now, request captcha audio\n",
    "            payload = {'state.action':'captcha','captcha_data.mode':'mixed-audio'}\n",
    "            payload.update({'session.sessionid': session_id})\n",
    "            url = 'https://www.bundesanzeiger.de/ebanzwww/contentloader'\n",
    "            try:\n",
    "                result = session_requests.get(url, params = payload)\n",
    "            except Exception as e:\n",
    "                print('Error in getting audio captcha!', repr(e))\n",
    "            if debug_prints:\n",
    "                print(\"get_captcha url\", result.url)\n",
    "                print(\"get_captcha returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "            if result.headers['content-type'] == \"audio/wav\":\n",
    "\n",
    "                # here the audio could be saved as a file:\n",
    "                if save_captcha_files:\n",
    "                    try:\n",
    "                        with open('scraped_data/captchas/captcha.wav', 'w+b') as file:\n",
    "                            file.write(result.content)\n",
    "                    except Exception as e:\n",
    "                        print(\"saving captcha.wav failed!\", repr(e))\n",
    "                        pass\n",
    "\n",
    "                print('Please solve this captcha. To cancel, type \"exit\". To replay, type \"replay\".')\n",
    "                captcha_solution = \"replay\"\n",
    "                # play the audio captcha to the user\n",
    "                while captcha_solution == \"replay\":\n",
    "                    wave_obj = sa.WaveObject.from_wave_file(BytesIO(result.content))\n",
    "                    play_obj = wave_obj.play()\n",
    "                    captcha_solution = solver.solve_and_return_solution('scraped_data/captchas/captcha.jpg')\n",
    "                    if captcha_solution != 0:\n",
    "                        print(\"captcha text \"+captcha_solution)\n",
    "                    else:\n",
    "                        print(\"task finished with error \"+solver.error_code)\n",
    "                        captcha_solution = ''\n",
    "                    play_obj.wait_done()\n",
    "                    if debug_prints:\n",
    "                        print(\"your solution:\", captcha_solution)\n",
    "                return captcha_solution\n",
    "            else:\n",
    "                print(\"no audio returned\")\n",
    "                return \"\"\n",
    "        else:\n",
    "            print(\"no image returned\")\n",
    "            return \"\"\n",
    "    else:\n",
    "        print('get_image_and_audio_captcha returned an error!')\n",
    "        return ''\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## get_document\n",
    "\n",
    "Mit dieser Funktion werden Dokumente abgerufen und deren HTML-Inhalt zurückgegeben. Dabei wird auch das vorgeschaltete Captcha beachtet und über get_captcha dem Nutzer gezeigt und abgefragt.\n",
    "\n",
    "Diese Funktion wird direkt aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_document(search_list_selected, search_list_destHistoryId, save_captcha_files, get_audio_captcha):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "    keepcharacters = (' ','.','_', '-')\n",
    "    retry_counter = 0\n",
    "    payload = {'page.navid':'detailsearchlisttodetailsearchdetail'}\n",
    "    payload.update({'session.sessionid': session_id})\n",
    "    payload.update({'fts_search_list.selected': search_list_selected})\n",
    "    payload.update({'fts_search_list.destHistoryId': search_list_destHistoryId})\n",
    "    payload.update({'captcha_data.mode': 'mixed-image'})\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/wexsservlet'\n",
    "    # time.sleep(2) # wait 2 seconds before the next request, to not overwhelm the server\n",
    "    try:\n",
    "        result = session_requests.get(url, params = payload)\n",
    "    except Exception as e:\n",
    "        print('Error in getting initial documant page!', repr(e))\n",
    "        result = None\n",
    "    if debug_prints:\n",
    "        print(\"get_details url:\", result.url)\n",
    "        print(\"get_details returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "    if result is not None:\n",
    "        # Parse the html content\n",
    "        soup = BeautifulSoup(result.text, \"lxml\")\n",
    "        # Parse for session id, update variable\n",
    "        session_id = soup.find('a', href=True)['href']\n",
    "        session_id = session_id[session_id.find(\"session.sessionid=\")+len(\"session.sessionid=\"):session_id.find(\"&\", session_id.find(\"session.sessionid=\"))]\n",
    "        # check if page has a captcha\n",
    "        if soup.find(\"div\", attrs={\"class\": \"image_captcha\"}) is not None :\n",
    "            #print(\"Captcha found\")\n",
    "            captcha_found = True\n",
    "            captcha_solution = \"\"\n",
    "            if get_audio_captcha:\n",
    "                captcha_solution = get_image_and_audio_captcha(session_id, save_captcha_files)\n",
    "            else:\n",
    "                captcha_solution = get_image_captcha(session_id, save_captcha_files)\n",
    "            #if captcha_solution == \"\" or captcha_solution == \"exit\":\n",
    "            if captcha_solution.lower() == \"exit\":\n",
    "                # if captcha was not solved\n",
    "                return \"exit\"\n",
    "            else:\n",
    "                # if captcha was solved, solution will be posted to server and response will be checked for new captcha\n",
    "                while captcha_found == True:\n",
    "                    # POST captcha solution\n",
    "                    post_payload = {\"genericsearch_param.part_id\":\"\",\"(page.navid=detailsearchdetailtodetailsearchdetailsolvecaptcha)\":\"OK\"}\n",
    "                    post_payload.update({'session.sessionid': session_id})\n",
    "                    post_payload.update({'captcha_data.solution': captcha_solution})\n",
    "                    post_url = 'https://www.bundesanzeiger.de/ebanzwww/wexsservlet'\n",
    "                    try:\n",
    "                        result = session_requests.post(post_url, data = post_payload)\n",
    "                        retry_counter = 0\n",
    "                    except Exception as e:\n",
    "                        print('Error in posting captcha!', repr(e))\n",
    "                        print('retrying now...')\n",
    "                        retry_counter = retry_counter + 1\n",
    "                        if retry_counter < 4:\n",
    "                            continue\n",
    "                    if debug_prints:\n",
    "                        print(\"post_captcha url: \", result.url)\n",
    "                        print(\"post_captcha returned: \", result.status_code, result.headers['content-type'])\n",
    "                    # Check for captcha again\n",
    "                    soup = BeautifulSoup(result.text, \"lxml\")\n",
    "                    # Parse for session id, update variable\n",
    "                    session_id = soup.find('a', href=True)['href']\n",
    "                    session_id = session_id[session_id.find(\"session.sessionid=\")+len(\"session.sessionid=\"):session_id.find(\"&\", session_id.find(\"session.sessionid=\"))]\n",
    "                    # check if page has a captcha again\n",
    "                    if soup.find(\"div\", attrs={\"class\": \"image_captcha\"}) is not None :\n",
    "                        print(\"WRONG captcha, new captcha found -- PLEASE TRY AGAIN\")\n",
    "                        captcha_found = True\n",
    "                        captcha_solution = \"\"\n",
    "                        if get_audio_captcha:\n",
    "                            captcha_solution = get_image_and_audio_captcha(session_id, save_captcha_files)\n",
    "                        else:\n",
    "                            captcha_solution = get_image_captcha(session_id, save_captcha_files)\n",
    "                        #if captcha_solution == \"\" or captcha_solution == \"exit\":\n",
    "                        if captcha_solution == \"exit\":\n",
    "                            return \"exit\"\n",
    "                        # ... if a solution was provided by the user, the loop then starts again\n",
    "                    else:\n",
    "                        captcha_found = False\n",
    "                        print(\"Captcha successfully solved!\")\n",
    "                        if save_captcha_files:\n",
    "                            try:\n",
    "                                os.rename(\"scraped_data/captchas/captcha.jpg\", \"scraped_data/captchas/captcha_\"+captcha_solution+\".jpg\")\n",
    "                                os.rename(\"scraped_data/captchas/captcha.wav\", \"scraped_data/captchas/captcha_\"+captcha_solution+\".wav\")\n",
    "                            except Exception as e:\n",
    "                                pass\n",
    "                        return soup\n",
    "                    # ... loop starts again\n",
    "                return soup\n",
    "        else:\n",
    "            return soup\n",
    "    else:\n",
    "        print('get_document returned an error!')\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weitere Funktionen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrape single company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# this function gets the result of the single company\n",
    "def get_single_company(company, place, from_year, print_title, save_captcha_files, get_audio_captcha, retry_counter=0) -> bool:\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "\n",
    "    # for company name sanitizing\n",
    "    keepcharacters = (' ','.','_', '-')\n",
    "    regex1 = re.compile(r'bis zum \\d{2}\\.\\d{2}\\.(\\d{4})')\n",
    "    csv_filename = 'scraped_data/'+\"\".join(c for c in company if c.isalnum() or c in keepcharacters).rstrip()+'.csv'\n",
    "    abort_execution = False\n",
    "    rerun_function = False\n",
    "    document_counter = 0\n",
    "    current_counter = 0\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(print_title)\n",
    "    print(\"COMPANY:\", company, '/', place,'/ earliest review', from_year)\n",
    "\n",
    "    # get search results\n",
    "    try:\n",
    "        df = get_search_results(company)\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            df.to_csv(csv_filename, index=False, encoding='utf-8', sep=';', quoting=csv.QUOTE_ALL)\n",
    "    except Exception as e:\n",
    "        print('Dataframe could not be saved as CSV!', repr(e))\n",
    "        df = None\n",
    "        pass\n",
    "\n",
    "\n",
    "    # count how many douments of search results are to be fetched\n",
    "    if df is not None and isinstance(df, pd.DataFrame):\n",
    "        for index, row in df.iterrows():\n",
    "            if row[0] != \"Suche - kein Suchergebnis\" and isinstance(row[2], str):\n",
    "                t = regex1.search(row[2])\n",
    "                if t is not None:\n",
    "                    if int(t.group()[-4:]) >= from_year:\n",
    "                        document_counter = document_counter + 1\n",
    "\n",
    "    # get documents of search results\n",
    "    if document_counter > 1:\n",
    "        if df is not None and isinstance(df, pd.DataFrame):\n",
    "            for index, row in df.iterrows():\n",
    "                if row[0] != \"Suche - kein Suchergebnis\" and isinstance(row[2], str):\n",
    "                    # filename like '%searchstring%_%documentdate%.html'\n",
    "                    html_filename = \"\".join(c for c in company if c.isalnum() or c in keepcharacters).rstrip()+\"_\"+row[3]+\".html\"\n",
    "                    # if the file for the current document does NOT exist, request it\n",
    "                    # else, the document will not be requested -- this avoids double work\n",
    "                    if not os.path.exists(os.path.join('scraped_data', html_filename)):\n",
    "                        t = regex1.search(row[2])\n",
    "                        if t is not None:\n",
    "                            if int(t.group()[-4:]) >= from_year:\n",
    "                                current_counter = current_counter+1\n",
    "                                clear_output(wait=True)\n",
    "                                print(print_title)\n",
    "                                print(\"COMPANY:\", company, '/', place,'/ earliest review', from_year)\n",
    "                                print('NEXT DOCUMENT (', current_counter, '/', document_counter, '):')\n",
    "                                print('>', row[0])\n",
    "                                print('> Dokument vom', row[3])\n",
    "                                print('>', row[2], '\\n')\n",
    "                                if row[5] == session_id:\n",
    "                                    # here the document is requested:\n",
    "                                    html_result = get_document(row[6], row[7], save_captcha_files, get_audio_captcha)\n",
    "                                    if html_result == \"exit\":\n",
    "                                        abort_execution = True\n",
    "                                        print(\"Execution was aborted!\")\n",
    "                                        break\n",
    "                                    elif html_result != \"\":\n",
    "                                        # check for invalidity here\n",
    "                                        invalid_id = html_result.findAll(class_='invalid')\n",
    "                                        if invalid_id:\n",
    "                                            print('The returned document was invalid, there was some error on the server-side. Will try again now...')\n",
    "                                            time.sleep(0.5)\n",
    "                                            rerun_function = True\n",
    "                                            break\n",
    "                                        else: # if document is valid:\n",
    "                                            try:\n",
    "                                                # filename like '%searchstring%_%documentdate%.html'\n",
    "                                                with open(os.path.join('scraped_data', html_filename), \"w\", encoding='utf-8') as file:\n",
    "                                                    file.write(str(html_result))\n",
    "                                            except Exception as e:\n",
    "                                                print(\"could not save HTML file\", repr(e))\n",
    "\n",
    "                                else:\n",
    "                                    print(\"Session ID of searchresult does not match, entry will be skipped.\")\n",
    "                    else:\n",
    "                        current_counter = current_counter+1\n",
    "                        clear_output(wait=True)\n",
    "                        print(print_title)\n",
    "                        print(\"COMPANY:\", company, '/', place,'/ earliest review', from_year)\n",
    "                        print('NEXT DOCUMENT (', current_counter, '/', document_counter, '):')\n",
    "                        print('>', row[0])\n",
    "                        print('> Dokument vom', row[3])\n",
    "                        print('>', row[2], '\\n')\n",
    "                        print(\"HTML file already exists for this document!!\")\n",
    "                        # time.sleep(0.5)\n",
    "                else:\n",
    "                    print(\"No search result!\")\n",
    "                    # time.sleep(0.5)\n",
    "                    break\n",
    "        else:\n",
    "            print('No search result!')\n",
    "            # time.sleep(0.5)\n",
    "    else:\n",
    "        print('Too few documents in scope!')\n",
    "\n",
    "    if abort_execution:\n",
    "        return False\n",
    "\n",
    "    if rerun_function and retry_counter < 4: # will do once and then re-try three times\n",
    "        final_result = get_single_company(company=company, place=place, from_year=from_year,\n",
    "                                          print_title=print_title, save_captcha_files=save_captcha_files,\n",
    "                                          get_audio_captcha=get_audio_captcha, retry_counter=(retry_counter+1))\n",
    "        return final_result\n",
    "\n",
    "    print(\"Done for this company!\")\n",
    "    # time.sleep(0.5)\n",
    "\n",
    "    return True # means everything is OK\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ausführung:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Getting documents for company 6/6 of 13909 ===\n",
      "COMPANY: disquom funktechnik GmbH / Grafschaft / earliest review 2013\n",
      "Too few documents in scope!\n",
      "Done for this company!\n"
     ]
    }
   ],
   "source": [
    "filepath_merged_companies='company_timestamps.json'\n",
    "\n",
    "abort_execution = False\n",
    "\n",
    "# get company data from file\n",
    "try:\n",
    "    with open(filepath_merged_companies) as json_file:\n",
    "        company_names = json.load(json_file)\n",
    "        merged_companies_len = len(company_names)\n",
    "except Exception as e:\n",
    "    print(\"couldn't read JSON file!\", repr(e))\n",
    "    pass\n",
    "\n",
    "# get counter from file\n",
    "try:\n",
    "    with open('current_index.json', 'r') as file:\n",
    "        save_list = json.load(file)\n",
    "    counter_curr = save_list[0]\n",
    "    counter_to = save_list[1]\n",
    "except Exception as e:\n",
    "    counter_curr = 0\n",
    "    counter_to = merged_companies_len\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loop through all companies\n",
    "for item in company_names:\n",
    "    if counter_curr <= company_names.index(item) < counter_to:\n",
    "\n",
    "        # get all douments for single company, save return value in variable\n",
    "        abort_execution = not get_single_company(from_year=int(item[1]), company=item[0].strip(), place=item[4].strip(),\n",
    "                                                 print_title=\"=== Getting documents for company \"+str(company_names.index(item)+1)+\"/\"+str(counter_to)+' of '+str(merged_companies_len)+\" ===\",\n",
    "                                                 save_captcha_files=True, get_audio_captcha=False)\n",
    "\n",
    "        if not abort_execution:\n",
    "            counter_curr = counter_curr + 1\n",
    "            # save current index\n",
    "            try:\n",
    "                with open('current_index.json', 'w') as f:\n",
    "                    save_list = [counter_curr, counter_to]\n",
    "                    json.dump(save_list, f)\n",
    "            except Exception as e:\n",
    "                print(\"index saving failed\", repr(e))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "if abort_execution:\n",
    "    print('Execution was stopped.')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}