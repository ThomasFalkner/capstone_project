{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bundesanzeiger Scraping\n",
    "\n",
    "Der folgende Code scraped die Webseite https://www.bundesanzeiger.de/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for html network requests and parsing:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "# for displaying the captcha images:\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "# for file importing and exporting:\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "# for waiting: \n",
    "from datetime import datetime\n",
    "import time\n",
    "# for audio:\n",
    "import simpleaudio as sa\n",
    "# other:\n",
    "import re # regex\n",
    "#from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globale Variablen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_requests = requests.session()\n",
    "session_id = \"\"\n",
    "debug_prints = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basisfunktionen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_search_results\n",
    "\n",
    "Diese Funktion stellt eine Anfrage an die Suchmaske von https://www.bundesanzeiger.de/ und gibt die Ergebnistabelle als Dataframe zurück.\n",
    "\n",
    "Diese Funktion wird direkt aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_search_results(search_string):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "    # genericsearch_param.part_id: 22 is for just Jahresabschlüsse\n",
    "    payload = {'page.navid': 'to_detailsearch', 'global_data.designmode': 'eb', '(page.navid=to_quicksearchlist)': 'Suchen', 'genericsearch_param.part_id': '22', 'genericsearch_param.hitsperpage': '20'}\n",
    "    payload.update({'genericsearch_param.fulltext': search_string})\n",
    "    if session_id != \"\":\n",
    "        payload.update({'session.sessionid': session_id})\n",
    "\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/wexsservlet'\n",
    "    time.sleep(0.1) # wait some seconds before the next request, to not overwhelm the server\n",
    "    try:\n",
    "        result = session_requests.get(url, params = payload)\n",
    "    except Exception as e:\n",
    "        print('Error in getting search result!', repr(e))\n",
    "        result = None\n",
    "    if debug_prints:\n",
    "        print(\"get_searchresults url:\", result.url)\n",
    "        print(\"get_searchresults returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "    if result is not None:\n",
    "        # Get html content\n",
    "        soup = BeautifulSoup(result.text, \"lxml\")\n",
    "        # Parse for session id, update variable\n",
    "        session_id = soup.find('a', href=True)['href']\n",
    "        session_id = session_id[session_id.find(\"session.sessionid=\")+len(\"session.sessionid=\"):session_id.find(\"&\", session_id.find(\"session.sessionid=\"))]\n",
    "        if debug_prints:\n",
    "            print(\"session_id:\", session_id)\n",
    "\n",
    "        # Parse html content\n",
    "        result_table = soup.find(\"table\", attrs={\"class\": \"result\", \"summary\": \"Trefferliste\"})\n",
    "        rows = result_table.find_all(\"tr\")\n",
    "        table_contents = []\n",
    "        for tr in rows:\n",
    "            # header row\n",
    "            if rows.index(tr) == 0 :\n",
    "                row_cells = [ th.getText(separator=\" \").strip() for th in tr.find_all('th') if th.getText().strip() != '' ] + [\"session.sessionid=\", \"fts_search_list.selected\", \"fts_search_list.destHistoryId\", \"timestamp\"]\n",
    "            # data rows\n",
    "            else:\n",
    "                # get 'th' element text\n",
    "                row_cells = ([ tr.find('th').getText(separator=\" \") ] if tr.find('th') else [] ) + \\\n",
    "                [ td.getText(separator=\" \").strip() for td in tr.find_all('td') if td.getText().strip() != '' ] + \\\n",
    "                [ a['href'][a['href'].find(\"session.sessionid=\")+len(\"session.sessionid=\"):a['href'].find(\"&\", a['href'].find(\"session.sessionid=\"))] for a in tr.find_all('a', href=True)] + \\\n",
    "                [ a['href'][a['href'].find(\"fts_search_list.selected=\")+len(\"fts_search_list.selected=\"):a['href'].find(\"&\", a['href'].find(\"fts_search_list.selected=\"))] for a in tr.find_all('a', href=True)] + \\\n",
    "                [ a['href'][a['href'].find(\"fts_search_list.destHistoryId=\")+len(\"fts_search_list.destHistoryId=\"):] for a in tr.find_all('a', href=True)] + \\\n",
    "                [ time.ctime() ]\n",
    "            if len(row_cells) > 1 :\n",
    "                table_contents += [ row_cells ]\n",
    "\n",
    "        # Convert to dataframe and set first row as headers\n",
    "        df = pd.DataFrame.from_dict(table_contents)\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df[1:]\n",
    "        return df\n",
    "    else:\n",
    "        print('get_search_results had an erorr!')\n",
    "        df = pd.DataFrame()\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weitere Funktionen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrape single company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# this function gets the result of the single company\n",
    "def count_documents_of_company(company, from_year, print_title):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "\n",
    "    # for company name sanitizing\n",
    "    keepcharacters = (' ','.','_', '-')\n",
    "    regex1 = re.compile(r'bis zum \\d{2}\\.\\d{2}\\.(\\d{4})')\n",
    "    csv_filename = 'scraped_data_counting/'+\"\".join(c for c in company if c.isalnum() or c in keepcharacters).rstrip()+'.csv'\n",
    "    abort_execution = False\n",
    "    rerun_function = False\n",
    "    document_counter = 0\n",
    "    current_counter = 0\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(print_title)\n",
    "\n",
    "    # get search results\n",
    "    try:\n",
    "        df = get_search_results(company)\n",
    "    except Exception as e:\n",
    "        print('get_seach_results returned error!', repr(e))\n",
    "        df = None\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            df.to_csv(csv_filename, index=False, encoding='utf-8', sep=';', quoting=csv.QUOTE_ALL)\n",
    "    except Exception as e:\n",
    "        print('Dataframe could not be saved as CSV!', repr(e))\n",
    "        df = None\n",
    "        pass\n",
    "\n",
    "\n",
    "    # count how many douments of search results are to be fetched\n",
    "    if df is not None and isinstance(df, pd.DataFrame):\n",
    "        for index, row in df.iterrows():\n",
    "            if row[0] != \"Suche - kein Suchergebnis\" and isinstance(row[2], str):\n",
    "                t = regex1.search(row[2])\n",
    "                if t is not None:\n",
    "                    if int(t.group()[-4:]) >= from_year:\n",
    "                        document_counter = document_counter + 1\n",
    "    print('>', document_counter, 'documents in scope!')\n",
    "    return document_counter\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ausführung:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Getting documents for company 13908/13909 ===\n",
      "===> documents currently in scope: 52700\n",
      "> 2 documents in scope!\n",
      "> Done! 52702 documents in scope!\n"
     ]
    }
   ],
   "source": [
    "filepath_merged_companies='../company_timestamps.json'\n",
    "\n",
    "abort_execution = False\n",
    "\n",
    "# get company data from file\n",
    "try:\n",
    "    with open(filepath_merged_companies) as json_file:\n",
    "        company_names = json.load(json_file)\n",
    "        merged_companies_len = len(company_names)\n",
    "except Exception as e:\n",
    "    print(\"couldn't read JSON file!\", repr(e))\n",
    "    pass\n",
    "\n",
    "\n",
    "# get counter from file\n",
    "try:\n",
    "    with open('current_index.json', 'r') as file:\n",
    "        save_list = json.load(file)\n",
    "    counter_walking = save_list[0]\n",
    "    counter_overall = save_list[1]\n",
    "except Exception as e:\n",
    "    counter_walking = 0\n",
    "    counter_overall = 0\n",
    "\n",
    "counter_curr = 0\n",
    "\n",
    "\n",
    "# counter_walking = 0\n",
    "# counter_overall = 0\n",
    "\n",
    "counted_documents = []\n",
    "\n",
    "# loop through all companies\n",
    "for item in company_names:\n",
    "    if counter_walking <= company_names.index(item):\n",
    "        # get all douments for single comp any, save return value in variable\n",
    "        counter_curr = count_documents_of_company(from_year=int(item[1]), company=item[0].strip(), print_title='=== Getting documents for company '+str(company_names.index(item))+'/'+str(merged_companies_len)+' ==='+'\\n'+'===> documents currently in scope: '+str(counter_overall))\n",
    "        counter_overall = counter_overall + counter_curr\n",
    "        counted_documents.append((item[0].strip(), counter_curr))\n",
    "        counter_walking = counter_walking + 1\n",
    "        if counter_walking % 500 == 0:\n",
    "            try:\n",
    "                with open('counted_documents.json', 'w') as f:\n",
    "                    json.dump(counted_documents, f)\n",
    "            except Exception as e:\n",
    "                print(\"saving counted_documents failed\", repr(e))\n",
    "\n",
    "            try:\n",
    "                with open('counted_documents_sum.json', 'w') as f:\n",
    "                    json.dump(counter_overall, f)\n",
    "            except Exception as e:\n",
    "                print(\"saving counted_documents_sum failed\", repr(e))\n",
    "\n",
    "            try:\n",
    "                with open('current_index.json', 'w') as f:\n",
    "                    save_list = [counter_walking, counter_overall]\n",
    "                    json.dump(save_list, f)\n",
    "            except Exception as e:\n",
    "                print(\"index saving failed\", repr(e))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('> Done!', counter_overall, 'documents in scope!')\n",
    "try:\n",
    "    with open('counted_documents.json', 'w') as f:\n",
    "        json.dump(counted_documents, f)\n",
    "except Exception as e:\n",
    "    print(\"saving counted_documents failed\", repr(e))\n",
    "\n",
    "try:\n",
    "    with open('counted_documents_sum.json', 'w') as f:\n",
    "        json.dump(counter_overall, f)\n",
    "except Exception as e:\n",
    "    print(\"saving counted_documents_sum failed\", repr(e))\n",
    "\n",
    "try:\n",
    "    with open('current_index.json', 'w') as f:\n",
    "        save_list = [counter_walking, counter_overall]\n",
    "        json.dump(save_list, f)\n",
    "except Exception as e:\n",
    "    print(\"index saving failed\", repr(e))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13908\n",
      "['wildstyle network GmbH', 12]\n",
      "       Documents in scope\n",
      "count        13908.000000\n",
      "mean             3.789330\n",
      "std              4.817626\n",
      "min              0.000000\n",
      "25%              0.000000\n",
      "50%              2.000000\n",
      "75%              6.000000\n",
      "max             20.000000\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('counted_documents.json', 'r') as file:\n",
    "        counted_documents = json.load(file)\n",
    "except Exception as e:\n",
    "    print('could not open counted_documents.json!')\n",
    "\n",
    "cols = ['Company', 'Documents in scope']\n",
    "\n",
    "print(len(counted_documents))\n",
    "print(counted_documents[0])\n",
    "\n",
    "df = pd.DataFrame(counted_documents, columns=['Company', 'Documents in scope'])\n",
    "\n",
    "print(df.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}