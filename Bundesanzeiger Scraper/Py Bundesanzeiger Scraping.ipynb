{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bundesanzeiger Scraping\n",
    "\n",
    "Der folgende Code scraped die Webseite https://www.bundesanzeiger.de/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for html network requests and parsing:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "# for displaying the captcha images:\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "# for file importing and exporting:\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "# for waiting: \n",
    "from datetime import datetime\n",
    "import time\n",
    "# for audio:\n",
    "import simpleaudio as sa\n",
    "# other:\n",
    "import re # regex\n",
    "#from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globale Variablen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_requests = requests.session()\n",
    "session_id = \"\"\n",
    "debug_prints = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basisfunktionen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_search_results\n",
    "\n",
    "Diese Funktion stellt eine Anfrage an die Suchmaske von https://www.bundesanzeiger.de/ und gibt die Ergebnistabelle als Dataframe zurück.\n",
    "\n",
    "Diese Funktion wird direkt aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_search_results(search_string):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "    # genericsearch_param.part_id: 22 is for just Jahresabschlüsse\n",
    "    payload = {'page.navid': 'to_detailsearch', 'global_data.designmode': 'eb', '(page.navid=to_quicksearchlist)': 'Suchen', 'genericsearch_param.part_id': '22', 'genericsearch_param.hitsperpage': '10'}\n",
    "    payload.update({'genericsearch_param.fulltext': search_string})\n",
    "    if session_id != \"\":\n",
    "        payload.update({'session.sessionid': session_id})\n",
    "\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/wexsservlet'\n",
    "    time.sleep(0.33) # wait some seconds before the next request, to not overwhelm the server\n",
    "    result = session_requests.get(url, params = payload)\n",
    "    if debug_prints:\n",
    "        print(\"get_searchresults url:\", result.url)\n",
    "        print(\"get_searchresults returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "    # Get html content\n",
    "    soup = BeautifulSoup(result.text, \"lxml\")\n",
    "    # Parse for session id, update variable\n",
    "    session_id = soup.find('a', href=True)['href']\n",
    "    session_id = session_id[session_id.find(\"session.sessionid=\")+len(\"session.sessionid=\"):session_id.find(\"&\", session_id.find(\"session.sessionid=\"))]\n",
    "    if debug_prints:\n",
    "        print(\"session_id:\", session_id)\n",
    "\n",
    "    # Parse html content \n",
    "    result_table = soup.find(\"table\", attrs={\"class\": \"result\", \"summary\": \"Trefferliste\"})\n",
    "    rows = result_table.find_all(\"tr\")\n",
    "    table_contents = []\n",
    "    for tr in rows:\n",
    "        # header row\n",
    "        if rows.index(tr) == 0 : \n",
    "            row_cells = [ th.getText(separator=\" \").strip() for th in tr.find_all('th') if th.getText().strip() != '' ] + [\"session.sessionid=\", \"fts_search_list.selected\", \"fts_search_list.destHistoryId\", \"timestamp\"] \n",
    "        # data rows\n",
    "        else:\n",
    "            # get 'th' element text\n",
    "            row_cells = ([ tr.find('th').getText(separator=\" \") ] if tr.find('th') else [] ) + \\\n",
    "            [ td.getText(separator=\" \").strip() for td in tr.find_all('td') if td.getText().strip() != '' ] + \\\n",
    "            [ a['href'][a['href'].find(\"session.sessionid=\")+len(\"session.sessionid=\"):a['href'].find(\"&\", a['href'].find(\"session.sessionid=\"))] for a in tr.find_all('a', href=True)] + \\\n",
    "            [ a['href'][a['href'].find(\"fts_search_list.selected=\")+len(\"fts_search_list.selected=\"):a['href'].find(\"&\", a['href'].find(\"fts_search_list.selected=\"))] for a in tr.find_all('a', href=True)] + \\\n",
    "            [ a['href'][a['href'].find(\"fts_search_list.destHistoryId=\")+len(\"fts_search_list.destHistoryId=\"):] for a in tr.find_all('a', href=True)] + \\\n",
    "            [ time.ctime() ]\n",
    "        if len(row_cells) > 1 : \n",
    "            table_contents += [ row_cells ]\n",
    "    \n",
    "    # Convert to dataframe and set first row as headers\n",
    "    df = pd.DataFrame.from_dict(table_contents)\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_captcha\n",
    "\n",
    "Bevor eine Dokumentenanfrage mit dem gewünschen Ergebnis beantwortet wird, muss der User ein Captcha lösen. Hier wird die Lösung dem Nutzer gezeigt und seine Eingabe verlangt.\n",
    "\n",
    "Diese Funktion wird von get_document aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gets just the image captcha\n",
    "def get_image_captcha(session_id, save_captcha_files):\n",
    "    global debug_prints\n",
    "    payload = {'state.action':'captcha','captcha_data.mode':'image'}\n",
    "    payload.update({'session.sessionid': session_id})\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/contentloader'\n",
    "    try:\n",
    "        result = session_requests.get(url, params = payload)\n",
    "    except Exception as e:\n",
    "        print('Error!', repr(e))\n",
    "    if debug_prints:\n",
    "        print(\"get_captcha url\", result.url)\n",
    "        print(\"get_captcha returned:\", result.status_code, result.headers['content-type'])\n",
    "    \n",
    "    if result.headers['content-type'] == \"image/jpeg\":\n",
    "\n",
    "        ## here the image could be saved as a file:\n",
    "        if save_captcha_files:\n",
    "            try:\n",
    "                with open('scraped_data/captcha.jpg', 'w+b') as file:\n",
    "                    file.write(result.content)\n",
    "            except Exception as e:\n",
    "                print(\"saving captcha.jpg failed!\", repr(e))\n",
    "                pass\n",
    "\n",
    "        img = Image.open(BytesIO(result.content))\n",
    "        display(img)\n",
    "        print('Please solve this captcha. To cancel, type \"exit\"')\n",
    "        captcha_solution = \"\"\n",
    "        captcha_solution = input()\n",
    "        return captcha_solution\n",
    "    else:\n",
    "        print(\"no image returned\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# first get a image and then audio captcha, else service doesn't correctly process request\n",
    "def get_image_and_audio_captcha(session_id, save_captcha_files):\n",
    "    global debug_prints\n",
    "    payload = {'state.action':'captcha','captcha_data.mode':'mixed-image'}\n",
    "    payload.update({'session.sessionid': session_id})\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/contentloader'\n",
    "    # here the captcha image is requested\n",
    "    try:\n",
    "        result = session_requests.get(url, params = payload)\n",
    "    except Exception as e:\n",
    "        print('Error!', repr(e))\n",
    "    if debug_prints:\n",
    "        print(\"get_captcha url\", result.url)\n",
    "        print(\"get_captcha returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "    if result.headers['content-type'] == \"image/jpeg\":\n",
    "\n",
    "        # here the image could be saved as a file:\n",
    "        if save_captcha_files:\n",
    "            try:\n",
    "                with open('scraped_data/captcha.jpg', 'w+b') as file:\n",
    "                    file.write(result.content)\n",
    "            except Exception as e:\n",
    "                print(\"saving captcha.jpg failed!\", repr(e))\n",
    "                pass\n",
    "\n",
    "        img = Image.open(BytesIO(result.content))\n",
    "        display(img)\n",
    "        # now, request captcha audio\n",
    "        payload = {'state.action':'captcha','captcha_data.mode':'mixed-audio'}\n",
    "        payload.update({'session.sessionid': session_id})\n",
    "        url = 'https://www.bundesanzeiger.de/ebanzwww/contentloader'\n",
    "        try:\n",
    "            result = session_requests.get(url, params = payload)\n",
    "        except Exception as e:\n",
    "            print('Error!', repr(e))\n",
    "        if debug_prints:\n",
    "            print(\"get_captcha url\", result.url)\n",
    "            print(\"get_captcha returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "        if result.headers['content-type'] == \"audio/wav\":\n",
    "\n",
    "            # here the audio could be saved as a file:\n",
    "            if save_captcha_files:\n",
    "                try:\n",
    "                    with open('scraped_data/captcha.wav', 'w+b') as file:\n",
    "                        file.write(result.content)\n",
    "                except Exception as e:\n",
    "                    print(\"saving captcha.wav failed!\", repr(e))\n",
    "                    pass\n",
    "\n",
    "            print('Please solve this captcha. To cancel, type \"exit\". To replay, type \"replay\".')\n",
    "            captcha_solution = \"replay\"\n",
    "            # play the audio captcha to the user\n",
    "            while captcha_solution == \"replay\":\n",
    "                wave_obj = sa.WaveObject.from_wave_file(BytesIO(result.content))\n",
    "                play_obj = wave_obj.play()\n",
    "                captcha_solution = input()\n",
    "                play_obj.wait_done()\n",
    "                if debug_prints:\n",
    "                    print(\"your solution:\", captcha_solution)\n",
    "            return captcha_solution\n",
    "        else:\n",
    "            print(\"no audio returned\")\n",
    "            return \"\"\n",
    "    else:\n",
    "        print(\"no image returned\")\n",
    "        return \"\"\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## get_document\n",
    "\n",
    "Mit dieser Funktion werden Dokumente abgerufen und deren HTML-Inhalt zurückgegeben. Dabei wird auch das vorgeschaltete Captcha beachtet und über get_captcha dem Nutzer gezeigt und abgefragt.\n",
    "\n",
    "Diese Funktion wird direkt aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_document(search_list_selected, search_list_destHistoryId, save_captcha_files, get_audio_captcha):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "    keepcharacters = (' ','.','_', '-')\n",
    "    retry_counter = 0\n",
    "    payload = {'page.navid':'detailsearchlisttodetailsearchdetail'}\n",
    "    payload.update({'session.sessionid': session_id})\n",
    "    payload.update({'fts_search_list.selected': search_list_selected})\n",
    "    payload.update({'fts_search_list.destHistoryId': search_list_destHistoryId})\n",
    "    payload.update({'captcha_data.mode': 'mixed-image'})\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/wexsservlet'\n",
    "    time.sleep(2) # wait 2 seconds before the next request, to not overwhelm the server\n",
    "    try:\n",
    "        result = session_requests.get(url, params = payload)\n",
    "    except Exception as e:\n",
    "        print('Error!', repr(e))\n",
    "    if debug_prints:\n",
    "        print(\"get_details url:\", result.url)\n",
    "        print(\"get_details returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "    # Parse the html content\n",
    "    soup = BeautifulSoup(result.text, \"lxml\")\n",
    "    # Parse for session id, update variable\n",
    "    session_id = soup.find('a', href=True)['href']\n",
    "    session_id = session_id[session_id.find(\"session.sessionid=\")+len(\"session.sessionid=\"):session_id.find(\"&\", session_id.find(\"session.sessionid=\"))]\n",
    "    # check if page has a captcha\n",
    "    if soup.find(\"div\", attrs={\"class\": \"image_captcha\"}) is not None :\n",
    "        #print(\"Captcha found\")\n",
    "        captcha_found = True\n",
    "        captcha_solution = \"\"\n",
    "        if get_audio_captcha:\n",
    "            captcha_solution = get_image_and_audio_captcha(session_id, save_captcha_files)\n",
    "        else:\n",
    "            captcha_solution = get_image_captcha(session_id, save_captcha_files)\n",
    "        #if captcha_solution == \"\" or captcha_solution == \"exit\":\n",
    "        if captcha_solution == \"exit\":\n",
    "            # if captcha was not solved\n",
    "            return \"exit\"\n",
    "        else:\n",
    "            # if captcha was solved, solution will be posted to server and response will be checked for new captcha\n",
    "            while captcha_found == True:\n",
    "                # POST captcha solution\n",
    "                post_payload = {\"genericsearch_param.part_id\":\"\",\"(page.navid=detailsearchdetailtodetailsearchdetailsolvecaptcha)\":\"OK\"}\n",
    "                post_payload.update({'session.sessionid': session_id})\n",
    "                post_payload.update({'captcha_data.solution': captcha_solution})\n",
    "                post_url = 'https://www.bundesanzeiger.de/ebanzwww/wexsservlet'\n",
    "                try:\n",
    "                    result = session_requests.post(post_url, data = post_payload)\n",
    "                    retry_counter = 0\n",
    "                except Exception as e:\n",
    "                    print('Error!', repr(e))\n",
    "                    retry_counter = retry_counter + 1\n",
    "                    if retry_counter < 4:\n",
    "                        continue\n",
    "                if debug_prints:\n",
    "                    print(\"post_captcha url: \", result.url)\n",
    "                    print(\"post_captcha returned: \", result.status_code, result.headers['content-type'])\n",
    "                # Check for captcha again\n",
    "                soup = BeautifulSoup(result.text, \"lxml\")\n",
    "                # Parse for session id, update variable\n",
    "                session_id = soup.find('a', href=True)['href']\n",
    "                session_id = session_id[session_id.find(\"session.sessionid=\")+len(\"session.sessionid=\"):session_id.find(\"&\", session_id.find(\"session.sessionid=\"))]\n",
    "                # check if page has a captcha again\n",
    "                if soup.find(\"div\", attrs={\"class\": \"image_captcha\"}) is not None :\n",
    "                    print(\"WRONG captcha, new captcha found -- PLEASE TRY AGAIN\")\n",
    "                    captcha_found = True\n",
    "                    captcha_solution = \"\"\n",
    "                    if get_audio_captcha:\n",
    "                        captcha_solution = get_image_and_audio_captcha(session_id, save_captcha_files)\n",
    "                    else:\n",
    "                        captcha_solution = get_image_captcha(session_id, save_captcha_files)\n",
    "                    #if captcha_solution == \"\" or captcha_solution == \"exit\":\n",
    "                    if captcha_solution == \"exit\":\n",
    "                        return \"exit\"\n",
    "                        break\n",
    "                    # ... if a solution was provided by the user, the loop then starts again\n",
    "                else:\n",
    "                    captcha_found = False\n",
    "                    print(\"Captcha successfully solved!\")\n",
    "                    if save_captcha_files:\n",
    "                        try:\n",
    "                            os.rename(\"scraped_data/captcha.jpg\", \"scraped_data/captcha_\"+captcha_solution+\".jpg\")\n",
    "                            os.rename(\"scraped_data/captcha.wav\", \"scraped_data/captcha_\"+captcha_solution+\".wav\")\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "                    return soup\n",
    "                # ... loop starts again \n",
    "            return soup\n",
    "    else:\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weitere Funktionen:\n",
    "\n",
    "Die folgenden Funktionen erledigen folgendes:\n",
    "* Funktion 1: get overview about companies from Kununu scraping\n",
    "    * Die Liste der Unternehmen eingelesen.\n",
    "    * Für diese Unternehmen wird die Suchfunktion ausgeführt, um die vorhandenen Dokumente bzw. Jahresabschlüsse zu scrapen.\n",
    "    * Die jeweiligen Suchergebnisse werden als csv exportiert. \n",
    "\n",
    "* Funktion 2: get documents of companies\n",
    "    * Die jeweiligen csv-Dateien werden eingelesen.\n",
    "    * Alle Jahresabschlüsse >= einer definierten Jahreszahl werden angefordert.\n",
    "    \n",
    "* Funktion 3: remove invalid documents\n",
    "    * Manchmal ist die Antwort des Servers, dass etwas schief gelaufen ist. Diese Dateien werden gelöscht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get overview about companies from kununu scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Liste aller Unternehmen\n",
    "def get_dnb_companies(from_index, to_index, filepath, print_title):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "\n",
    "    try:\n",
    "        with open(filepath) as json_file:\n",
    "            company_names = json.load(json_file)\n",
    "    except Exception as e:\n",
    "        print(\"couldn't read file!\")\n",
    "        pass\n",
    "\n",
    "    debug_prints = False\n",
    "    errors_occured = \"\"\n",
    "\n",
    "    # for company name sanitizing\n",
    "    keepcharacters = (' ','.','_', '-')\n",
    "\n",
    "    if 'company_names' in globals() or 'company_names' in locals():\n",
    "        for company in company_names[from_index:to_index]:\n",
    "            clear_output(wait=True)\n",
    "            print(print_title)\n",
    "            print(\"COMPANY:\", company_names.index(company)+1, \"/\", to_index, ' of ', len(company_names), \"--\", company)\n",
    "            # get search results\n",
    "            try:\n",
    "                if company != \"None\":\n",
    "                    df = get_search_results(company)\n",
    "                    df.to_csv('scraped_data/DNB_'+\"\".join(c for c in company if c.isalnum() or c in keepcharacters).rstrip()+\".csv\", \\\n",
    "                        index=False, encoding='utf-8', sep=';', quoting=csv.QUOTE_ALL)\n",
    "            except Exception as e:\n",
    "                errors_occurred = errors_occured + repr(e) + \"at company \" + company + \"\\n\"\n",
    "                if debug_prints:\n",
    "                    print('Some Error occured! Continue? y/y')\n",
    "                    print(e)\n",
    "                    test = input()\n",
    "                pass\n",
    "\n",
    "        print(\"Done!\")\n",
    "        if errors_occured != \"\":\n",
    "            print(\"The following errors occured:\")\n",
    "            print(errors_occured)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get documents of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_documents_of_saved_csvs(from_year, print_title, save_captcha_files, get_audio_captcha):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "\n",
    "    # get list of all files in folder 'scraped_data'\n",
    "    for root, dirs, files in os.walk('scraped_data'):\n",
    "        document_list = files\n",
    "        pass\n",
    "\n",
    "    abort_execution = False\n",
    "    skip_item = False\n",
    "\n",
    "    regex1 = re.compile(r'bis zum \\d{2}\\.\\d{2}\\.(\\d{4})')\n",
    "    current_filename = \"\"\n",
    "    if 'document_list' in globals() or 'document_list' in locals():\n",
    "        count_of_csvs = len([item for item in document_list if item.endswith('.csv')])\n",
    "    csv_counter = 1\n",
    "\n",
    "    # for company name sanitizing\n",
    "    keepcharacters = (' ','.','_', '-')\n",
    "\n",
    "    if 'document_list' in globals() or 'document_list' in locals():\n",
    "        for item in document_list:\n",
    "            clear_output(wait=True)\n",
    "            print(print_title)\n",
    "            print(\"COMPANY:\", csv_counter, \"/\", count_of_csvs, \"--\", item)\n",
    "            if item.endswith('.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv('scraped_data/'+item, sep=\";\")\n",
    "                    skip_item = False\n",
    "                except FileNotFoundError:\n",
    "                    print(\"file not found!\")\n",
    "                    skip_item = True\n",
    "            else:\n",
    "                skip_item = True\n",
    "\n",
    "            if not skip_item:\n",
    "                csv_counter = csv_counter + 1\n",
    "                for index, row in df.iterrows():\n",
    "                    if row[0] != \"Suche - kein Suchergebnis\" and isinstance(row[2], str):\n",
    "                        # filename like '%searchstring%_%documentdate%.html'\n",
    "                        current_filename = \"\".join(c for c in item if c.isalnum() or c in keepcharacters).rstrip()+\"_\"+row[3]+\".html\"\n",
    "                        # if the file for the current document does NOT exist, request it\n",
    "                        # else, the document will not be requested -- this avoids double work\n",
    "                        if not os.path.exists(os.path.join('scraped_data', current_filename)):\n",
    "                            t = regex1.search(row[2])\n",
    "                            if t is not None:\n",
    "                                if int(t.group()[-4:]) >= from_year:\n",
    "                                    clear_output(wait=True)\n",
    "                                    print(print_title)\n",
    "                                    print(\"COMPANY:\", csv_counter, \"/\", count_of_csvs, \"--\", item)\n",
    "                                    print(\"next:\", row[0], \"-- Dokument vom\", row[3], \"--\", row[2])\n",
    "                                    if row[5] == session_id:\n",
    "                                        # here the document is requested:\n",
    "                                        html_result = get_document(row[6], row[7], save_captcha_files, get_audio_captcha)\n",
    "                                        if html_result == \"exit\":\n",
    "                                            abort_execution = True\n",
    "                                            print(\"Execution was aborted!\")\n",
    "                                            break\n",
    "                                        elif html_result != \"\":\n",
    "                                            # filename like '%searchstring%_%documentdate%.html'\n",
    "                                            try:\n",
    "                                                with open(os.path.join('scraped_data', current_filename), \"w\", encoding='utf-8') as file:\n",
    "                                                    file.write(str(html_result))\n",
    "                                            except Exception as e:\n",
    "                                                print(\"could not save file\")\n",
    "                                    else:\n",
    "                                        print(\"Old searchresult, will be skipped.\")\n",
    "                        else:\n",
    "                            print(\"file already exists!\")\n",
    "                    else:\n",
    "                        break\n",
    "                if abort_execution:\n",
    "                    break\n",
    "\n",
    "\n",
    "        #   . for loop ends here\n",
    "    if abort_execution:\n",
    "        return True\n",
    "\n",
    "    print(\"Done!\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling: Removing html documents with content \"invalid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_invalid_htmls(print_title):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "\n",
    "    # get list of all files in folder 'scraped_data'\n",
    "    for root, dirs, files in os.walk('scraped_data'):\n",
    "        document_list = files\n",
    "\n",
    "    abort_execution = False\n",
    "    skip_item = False\n",
    "\n",
    "    count_of_htmls = len([item for item in document_list if item.endswith('.html')])+1\n",
    "    html_counter = 1\n",
    "    deleted_counter = 0\n",
    "\n",
    "    for item in document_list:\n",
    "        clear_output(wait=True)\n",
    "        print(print_title)\n",
    "        print(\"Scanning document:\", html_counter, \"/\", count_of_htmls, \"--\", item)\n",
    "\n",
    "        if item.endswith('.html'):\n",
    "            try:\n",
    "                with open('scraped_data/'+item) as file:\n",
    "                    soup = BeautifulSoup(file)\n",
    "                skip_item = False\n",
    "            except Exception as EError:\n",
    "                print(\"An Error occured!\", repr(EError))\n",
    "                skip_item = True\n",
    "        else:\n",
    "            skip_item = True\n",
    "\n",
    "        if not skip_item:\n",
    "            html_counter = html_counter + 1\n",
    "            invalid_id = soup.findAll(class_='invalid')\n",
    "            if invalid_id:\n",
    "                print('this document is invalid. Will be deleted now')\n",
    "                try:\n",
    "                    if os.path.isfile('scraped_data/'+item):\n",
    "                        os.remove('scraped_data/'+item)\n",
    "                        deleted_counter = deleted_counter+1\n",
    "                except Exception as e:\n",
    "                    print(\"An Error occured!\", repr(e))\n",
    "                    pass\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(\"Deleted\", deleted_counter, \"html files because they had invalid content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Other"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_count_of_beratungs_companies():\n",
    "    try:\n",
    "        with open('../DNB Scraper/Beratung1 (Livia)/company_names.json') as json_file:\n",
    "            company_names = json.load(json_file)\n",
    "            beratung_len = len(company_names)\n",
    "            return beratung_len\n",
    "    except Exception as e:\n",
    "        print(\"couldn't read Beratung company_names file!\")\n",
    "        pass\n",
    "\n",
    "def get_count_of_dienstleistungen_companies():\n",
    "    try:\n",
    "        with open('../DNB Scraper/Dienstleistungen1 (Philipp)/company_names.json') as json_file:\n",
    "            company_names = json.load(json_file)\n",
    "            dienstleistungen_len = len(company_names)\n",
    "            return dienstleistungen_len\n",
    "    except Exception as e:\n",
    "        print(\"couldn't read Dienstleistungen company_names file!\")\n",
    "        pass\n",
    "\n",
    "def get_count_of_it_companies():\n",
    "    try:\n",
    "        with open('../DNB Scraper/IT1 (Leon)/company_names.json') as json_file:\n",
    "            company_names = json.load(json_file)\n",
    "            it_len = len(company_names)\n",
    "            return it_len\n",
    "    except Exception as e:\n",
    "        print(\"couldn't read IT company_names file!\")\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ausführung:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Removing invalid HTMLS ===\n",
      "Scanning document: 94 / 97 -- DNB_Zukunftsagenten GmbH.csv\n",
      "Done!\n",
      "Deleted 0 html files because they had invalid content.\n",
      "execution was aborted!\n"
     ]
    }
   ],
   "source": [
    "beratung_len=get_count_of_beratungs_companies()\n",
    "dienstleistungen_len=get_count_of_dienstleistungen_companies()\n",
    "it_len=get_count_of_it_companies()\n",
    "\n",
    "abort_execution = False\n",
    "\n",
    "try:\n",
    "    with open('output/current_index.json', 'r') as file:\n",
    "        save_list = json.load(file)\n",
    "        counter_b = save_list[0]\n",
    "        counter_d = save_list[1]\n",
    "        counter_i = save_list[2]\n",
    "except Exception as e:\n",
    "    counter_b = 0\n",
    "    counter_d = 0\n",
    "    counter_i = 0\n",
    "\n",
    "SCHRITTLAENGE = 50\n",
    "\n",
    "# 1. Beratungs-Unternehmen\n",
    "while counter_b < beratung_len:\n",
    "    # 1. Get DNB Companies\n",
    "    if (counter_b+SCHRITTLAENGE) < beratung_len:\n",
    "        get_dnb_companies(from_index=counter_b, to_index=(counter_b+SCHRITTLAENGE-1), filepath='../DNB Scraper/Beratung1 (Livia)/company_names.json',  print_title=\"=== Getting DNB companies ===\")\n",
    "    else:\n",
    "        get_dnb_companies(from_index=counter_b, to_index=beratung_len, filepath='../DNB Scraper/Beratung1 (Livia)/company_names.json',  print_title=\"=== Getting DNB companies ===\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # 2. Get Documents of saved CSVs\n",
    "    abort_execution = get_documents_of_saved_csvs(from_year=2016, print_title=\"=== Getting documents of saved CSVs ===\", save_captcha_files=True, get_audio_captcha=False)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # 3. Remove invalid HTMLs\n",
    "    remove_invalid_htmls(print_title=\"=== Removing invalid HTMLS ===\")\n",
    "\n",
    "    if not abort_execution:\n",
    "        counter_b = counter_b + SCHRITTLAENGE\n",
    "\n",
    "        # save current index\n",
    "        try:\n",
    "            with open('output/current_index.json', 'w') as f:\n",
    "                save_list = [counter_b, counter_d, counter_i]\n",
    "                json.dump(save_list, f)\n",
    "        except Exception as e:\n",
    "            print(\"index saving failed\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Dienstleistungs-Unternehmen\n",
    "if not abort_execution:\n",
    "    while counter_d < dienstleistungen_len:\n",
    "        # 1. Get DNB Companies\n",
    "        if (counter_d+SCHRITTLAENGE) < dienstleistungen_len:\n",
    "            get_dnb_companies(from_index=counter_d, to_index=(counter_d+SCHRITTLAENGE-1), filepath='../DNB Scraper/Dienstleistungen1 (Philipp)/company_names.json',  print_title=\"=== Getting DNB companies ===\")\n",
    "        else:\n",
    "            get_dnb_companies(from_index=counter_d, to_index=dienstleistungen_len, filepath='../DNB Scraper/Dienstleistungen1 (Philipp)/company_names.json',  print_title=\"=== Getting DNB companies ===\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 2. Get Documents of saved CSVs\n",
    "        abort_execution = get_documents_of_saved_csvs(from_year=2016, print_title=\"=== Getting documents of saved CSVs ===\", save_captcha_files=True, get_audio_captcha=False)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 3. Remove invalid HTMLs\n",
    "        remove_invalid_htmls(print_title=\"=== Removing invalid HTMLS ===\")\n",
    "\n",
    "        if not abort_execution:\n",
    "            counter_d = counter_d + SCHRITTLAENGE\n",
    "\n",
    "            # save current index\n",
    "            try:\n",
    "                with open('output/current_index.json', 'w') as f:\n",
    "                    save_list = [counter_b, counter_d, counter_i]\n",
    "                    json.dump(save_list, f)\n",
    "            except Exception as e:\n",
    "                print(\"index saving failed\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "# 3. IT-Unternehmen\n",
    "if not abort_execution:\n",
    "    while counter_i < it_len:\n",
    "        # 1. Get DNB Companies\n",
    "        if (counter_i+SCHRITTLAENGE) < it_len:\n",
    "            get_dnb_companies(from_index=counter_i, to_index=(counter_i+SCHRITTLAENGE-1), filepath='../DNB Scraper/IT1 (Leon)/company_names.json',  print_title=\"=== Getting DNB companies ===\")\n",
    "        else:\n",
    "            get_dnb_companies(from_index=counter_i, to_index=it_len, filepath='../DNB Scraper/IT1 (Leon)/company_names.json',  print_title=\"=== Getting DNB companies ===\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 2. Get Documents of saved CSVs\n",
    "        abort_execution = get_documents_of_saved_csvs(from_year=2016, print_title=\"=== Getting documents of saved CSVs ===\", save_captcha_files=True, get_audio_captcha=False)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 3. Remove invalid HTMLs\n",
    "        remove_invalid_htmls(print_title=\"=== Removing invalid HTMLS ===\")\n",
    "\n",
    "        if not abort_execution:\n",
    "            counter_i = counter_i + SCHRITTLAENGE\n",
    "\n",
    "            # save current index\n",
    "            try:\n",
    "                with open('output/current_index.json', 'w') as f:\n",
    "                    save_list = [counter_b, counter_d, counter_i]\n",
    "                    json.dump(save_list, f)\n",
    "            except Exception as e:\n",
    "                print(\"index saving failed\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "if abort_execution:\n",
    "    print('execution was aborted!')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}