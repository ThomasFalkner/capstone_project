{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bundesanzeiger Scraping\n",
    "\n",
    "Der folgende Code scraped die Webseite https://www.bundesanzeiger.de/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for html network requests and parsing:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "# for displaying the captcha images:\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "# for file importing and exporting:\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "# for waiting: \n",
    "from datetime import datetime\n",
    "import time\n",
    "# for audio:\n",
    "import simpleaudio as sa\n",
    "# other:\n",
    "import re # regex\n",
    "#from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globale Variablen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_requests = requests.session()\n",
    "session_id = \"\"\n",
    "debug_prints = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funktionen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_search_results\n",
    "\n",
    "Diese Funktion stellt eine Anfrage an die Suchmaske von https://www.bundesanzeiger.de/ und gibt die Ergebnistabelle als Dataframe zurück.\n",
    "\n",
    "Diese Funktion wird direkt aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_search_results(search_string):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "    # genericsearch_param.part_id: 22 is for just Jahresabschlüsse\n",
    "    payload = {'page.navid': 'to_detailsearch', 'global_data.designmode': 'eb', '(page.navid=to_quicksearchlist)': 'Suchen', 'genericsearch_param.part_id': '22', 'genericsearch_param.hitsperpage': '10'}\n",
    "    payload.update({'genericsearch_param.fulltext': search_string})\n",
    "    if session_id != \"\":\n",
    "        payload.update({'session.sessionid': session_id})\n",
    "\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/wexsservlet'\n",
    "    time.sleep(0.33) # wait some seconds before the next request, to not overwhelm the server\n",
    "    result = session_requests.get(url, params = payload)\n",
    "    if debug_prints:\n",
    "        print(\"get_searchresults url:\", result.url)\n",
    "        print(\"get_searchresults returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "    # Get html content\n",
    "    soup = BeautifulSoup(result.text, \"lxml\")\n",
    "    # Parse for session id, update variable\n",
    "    session_id = soup.find('a', href=True)['href']\n",
    "    session_id = session_id[session_id.find(\"session.sessionid=\")+len(\"session.sessionid=\"):session_id.find(\"&\", session_id.find(\"session.sessionid=\"))]\n",
    "    if debug_prints:\n",
    "        print(\"session_id:\", session_id)\n",
    "\n",
    "    # Parse html content \n",
    "    result_table = soup.find(\"table\", attrs={\"class\": \"result\", \"summary\": \"Trefferliste\"})\n",
    "    rows = result_table.find_all(\"tr\")\n",
    "    table_contents = []\n",
    "    for tr in rows:\n",
    "        # header row\n",
    "        if rows.index(tr) == 0 : \n",
    "            row_cells = [ th.getText(separator=\" \").strip() for th in tr.find_all('th') if th.getText().strip() != '' ] + [\"session.sessionid=\", \"fts_search_list.selected\", \"fts_search_list.destHistoryId\", \"timestamp\"] \n",
    "        # data rows\n",
    "        else : \n",
    "            # get 'th' element text\n",
    "            row_cells = ([ tr.find('th').getText(separator=\" \") ] if tr.find('th') else [] ) + \\\n",
    "            [ td.getText(separator=\" \").strip() for td in tr.find_all('td') if td.getText().strip() != '' ] + \\\n",
    "            [ a['href'][a['href'].find(\"session.sessionid=\")+len(\"session.sessionid=\"):a['href'].find(\"&\", a['href'].find(\"session.sessionid=\"))] for a in tr.find_all('a', href=True)] + \\\n",
    "            [ a['href'][a['href'].find(\"fts_search_list.selected=\")+len(\"fts_search_list.selected=\"):a['href'].find(\"&\", a['href'].find(\"fts_search_list.selected=\"))] for a in tr.find_all('a', href=True)] + \\\n",
    "            [ a['href'][a['href'].find(\"fts_search_list.destHistoryId=\")+len(\"fts_search_list.destHistoryId=\"):] for a in tr.find_all('a', href=True)] + \\\n",
    "            [ time.ctime() ]\n",
    "        if len(row_cells) > 1 : \n",
    "            table_contents += [ row_cells ]\n",
    "    \n",
    "    # Convert to dataframe and set first row as headers\n",
    "    df = pd.DataFrame.from_dict(table_contents)\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_captcha\n",
    "\n",
    "Bevor eine Dokumentenanfrage mit dem gewünschen Ergebnis beantwortet wird, muss der User ein Captcha lösen. Hier wird die Lösung dem Nutzer gezeigt und seine Eingabe verlangt.\n",
    "\n",
    "Diese Funktion wird von get_document aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gets just the image captcha\n",
    "def get_image_captcha(session_id, save_captcha_files):\n",
    "    global debug_prints\n",
    "    payload = {'state.action':'captcha','captcha_data.mode':'image'}\n",
    "    payload.update({'session.sessionid': session_id})\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/contentloader'\n",
    "    result = session_requests.get(url, params = payload)\n",
    "    if debug_prints:\n",
    "        print(\"get_captcha url\", result.url)\n",
    "        print(\"get_captcha returned:\", result.status_code, result.headers['content-type'])\n",
    "    \n",
    "    if result.headers['content-type'] == \"image/jpeg\":\n",
    "        img = Image.open(BytesIO(result.content))\n",
    "        display(img)\n",
    "        print('Please solve this captcha. To cancel, type \"exit\"')\n",
    "        captcha_solution = \"\"\n",
    "        captcha_solution = input()\n",
    "        return captcha_solution\n",
    "    else:\n",
    "        print(\"no image returned\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# first get a image and then audio captcha, else service doesn't correctly process request\n",
    "def get_image_and_audio_captcha(session_id, save_captcha_files):\n",
    "    global debug_prints\n",
    "    payload = {'state.action':'captcha','captcha_data.mode':'mixed-image'}\n",
    "    payload.update({'session.sessionid': session_id})\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/contentloader'\n",
    "    # here the captcha image is requested\n",
    "    result = session_requests.get(url, params = payload)\n",
    "    if debug_prints:\n",
    "        print(\"get_captcha url\", result.url)\n",
    "        print(\"get_captcha returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "    if result.headers['content-type'] == \"image/jpeg\":\n",
    "\n",
    "        # here the image could be saved as a file:\n",
    "        if save_captcha_files:\n",
    "            try:\n",
    "                with open('scraped_data/captcha.jpg', 'w+b') as file:\n",
    "                    file.write(result.content)\n",
    "            except Exception as e:\n",
    "                print(\"saving captcha.jpg failed!\", repr(e))\n",
    "                pass\n",
    "\n",
    "        img = Image.open(BytesIO(result.content))\n",
    "        display(img)\n",
    "        # now, request captcha audio\n",
    "        payload = {'state.action':'captcha','captcha_data.mode':'mixed-audio'}\n",
    "        payload.update({'session.sessionid': session_id})\n",
    "        url = 'https://www.bundesanzeiger.de/ebanzwww/contentloader'\n",
    "        result = session_requests.get(url, params = payload)\n",
    "        if debug_prints:\n",
    "            print(\"get_captcha url\", result.url)\n",
    "            print(\"get_captcha returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "        if result.headers['content-type'] == \"audio/wav\":\n",
    "\n",
    "            # here the audio could be saved as a file:\n",
    "            if save_captcha_files:\n",
    "                try:\n",
    "                    with open('scraped_data/captcha.wav', 'w+b') as file:\n",
    "                        file.write(result.content)\n",
    "                except Exception as e:\n",
    "                    print(\"saving captcha.wav failed!\", repr(e))\n",
    "                    pass\n",
    "\n",
    "            print('Please solve this captcha. To cancel, type \"exit\". To replay, type \"replay\".')\n",
    "            captcha_solution = \"replay\"\n",
    "            # play the audio captcha to the user\n",
    "            while captcha_solution == \"replay\":\n",
    "                wave_obj = sa.WaveObject.from_wave_file(BytesIO(result.content))\n",
    "                play_obj = wave_obj.play()\n",
    "                captcha_solution = input()\n",
    "                play_obj.wait_done()\n",
    "                if debug_prints:\n",
    "                    print(\"your solution:\", captcha_solution)\n",
    "            return captcha_solution\n",
    "        else:\n",
    "            print(\"no audio returned\")\n",
    "            return \"\"\n",
    "    else:\n",
    "        print(\"no image returned\")\n",
    "        return \"\"\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_document\n",
    "\n",
    "Mit dieser Funktion werden Dokumente abgerufen und deren HTML-Inhalt zurückgegeben. Dabei wird auch das vorgeschaltete Captcha beachtet und über get_captcha dem Nutzer gezeigt und abgefragt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_document(search_list_selected, search_list_destHistoryId, save_captcha_files):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "    keepcharacters = (' ','.','_', '-')\n",
    "    payload = {'page.navid':'detailsearchlisttodetailsearchdetail'}\n",
    "    payload.update({'session.sessionid': session_id})\n",
    "    payload.update({'fts_search_list.selected': search_list_selected})\n",
    "    payload.update({'fts_search_list.destHistoryId': search_list_destHistoryId})\n",
    "    payload.update({'captcha_data.mode': 'mixed-image'})\n",
    "    url = 'https://www.bundesanzeiger.de/ebanzwww/wexsservlet'\n",
    "    time.sleep(2) # wait 2 seconds before the next request, to not overwhelm the server\n",
    "    result = session_requests.get(url, params = payload)\n",
    "    if debug_prints:\n",
    "        print(\"get_details url:\", result.url)\n",
    "        print(\"get_details returned:\", result.status_code, result.headers['content-type'])\n",
    "\n",
    "    # Parse the html content\n",
    "    soup = BeautifulSoup(result.text, \"lxml\")\n",
    "    # Parse for session id, update variable\n",
    "    session_id = soup.find('a', href=True)['href']\n",
    "    session_id = session_id[session_id.find(\"session.sessionid=\")+len(\"session.sessionid=\"):session_id.find(\"&\", session_id.find(\"session.sessionid=\"))]\n",
    "    # check if page has a captcha\n",
    "    if soup.find(\"div\", attrs={\"class\": \"image_captcha\"}) is not None :\n",
    "        #print(\"Captcha found\")\n",
    "        captcha_found = True\n",
    "        captcha_solution = \"\"\n",
    "        captcha_solution = get_image_and_audio_captcha(session_id, save_captcha_files)\n",
    "        # captcha_solution = get_captcha(session_id)\n",
    "        #if captcha_solution == \"\" or captcha_solution == \"exit\":\n",
    "        if captcha_solution == \"exit\":\n",
    "            # if captcha was not solved\n",
    "            return \"exit\"\n",
    "        else:\n",
    "            # if captcha was solved, solution will be posted to server and response will be checked for new captcha\n",
    "            while captcha_found == True:\n",
    "                # POST captcha solution\n",
    "                post_payload = {\"genericsearch_param.part_id\":\"\",\"(page.navid=detailsearchdetailtodetailsearchdetailsolvecaptcha)\":\"OK\"}\n",
    "                post_payload.update({'session.sessionid': session_id})\n",
    "                post_payload.update({'captcha_data.solution': captcha_solution})\n",
    "                post_url = 'https://www.bundesanzeiger.de/ebanzwww/wexsservlet'\n",
    "                result = session_requests.post(post_url, data = post_payload)\n",
    "                if debug_prints:\n",
    "                    print(\"post_captcha url: \", result.url)\n",
    "                    print(\"post_captcha returned: \", result.status_code, result.headers['content-type'])\n",
    "                # Check for captcha again\n",
    "                soup = BeautifulSoup(result.text, \"lxml\")\n",
    "                # Parse for session id, update variable\n",
    "                session_id = soup.find('a', href=True)['href']\n",
    "                session_id = session_id[session_id.find(\"session.sessionid=\")+len(\"session.sessionid=\"):session_id.find(\"&\", session_id.find(\"session.sessionid=\"))]\n",
    "                # check if page has a captcha again\n",
    "                if soup.find(\"div\", attrs={\"class\": \"image_captcha\"}) is not None :\n",
    "                    print(\"WRONG captcha, new captcha found -- PLEASE TRY AGAIN\")\n",
    "                    captcha_found = True\n",
    "                    captcha_solution = \"\"\n",
    "                    captcha_solution = get_image_and_audio_captcha(session_id, save_captcha_files)\n",
    "                    #if captcha_solution == \"\" or captcha_solution == \"exit\":\n",
    "                    if captcha_solution == \"exit\":\n",
    "                        return \"exit\"\n",
    "                    # ... if a solution was provided by the user, the loop then starts again\n",
    "                else:\n",
    "                    captcha_found = False\n",
    "                    print(\"Captcha successfully solved!\")\n",
    "                    if save_captcha_files:\n",
    "                        try:\n",
    "                            os.rename(\"scraped_data/captcha.jpg\", \"scraped_data/captcha_\"+captcha_solution+\".jpg\")\n",
    "                            os.rename(\"scraped_data/captcha.wav\", \"scraped_data/captcha_\"+captcha_solution+\".wav\")\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "                    return soup\n",
    "                # ... loop starts again \n",
    "            return soup\n",
    "    else:\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weitere Funktionen:\n",
    "\n",
    "Die folgenden Abschnitten machen folgendes:\n",
    "* Abschnitt 1: get overview about companies from Kununu scraping\n",
    "    * Die Liste der Unternehmen eingelesen.\n",
    "    * Für diese Unternehmen wird die Suchfunktion ausgeführt, um die vorhandenen Dokumente bzw. Jahresabschlüsse zu scrapen.\n",
    "    * Die jeweiligen Suchergebnisse werden als csv exportiert. \n",
    "    \n",
    "    \n",
    "* Abschnitt 2: get overview about companies from Insolvency scraping\n",
    "    * Die Liste der Unternehmen eingelesen.\n",
    "    * Für diese Unternehmen wird die Suchfunktion ausgeführt, um die vorhandenen Dokumente bzw. Jahresabschlüsse zu scrapen.\n",
    "    * Die jeweiligen Suchergebnisse werden als csv exportiert. \n",
    "    \n",
    "    \n",
    "* Abschnitt 3: get documents of companies\n",
    "    * Die jeweiligen csv-Dateien werden eingelesen.\n",
    "    * Alle Jahresabschlüsse >= 2018 werden angefordert.\n",
    "    \n",
    "* Abschnitt 4: remove invalid documents\n",
    "    * Manchmal ist die Antwort des Servers, dass etwas fehlerhaft ist. Diese Dateien werden gelöscht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get overview about companies from kununu scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Liste aller Unternehmen\n",
    "def get_kununu_companies(from_index, to_index, print_title):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "\n",
    "    with open('../DNB Scraper/IT1 (Leon)/company_names.json') as json_file:\n",
    "        company_names = json.load(json_file)\n",
    "\n",
    "\n",
    "    debug_prints = False\n",
    "    errors_occured = \"\"\n",
    "\n",
    "    # for company name sanitizing\n",
    "    keepcharacters = (' ','.','_', '-')\n",
    "\n",
    "    if 'company_names' in globals() or 'company_names' in locals():\n",
    "        for company in company_names[from_index:to_index]:\n",
    "            clear_output(wait=True)\n",
    "            print(print_title)\n",
    "            print(\"COMPANY:\", company_names.index(company)+1, \"/\", len(company_names), \"--\", company)\n",
    "            # get search results\n",
    "            try:\n",
    "                if company != \"None\":\n",
    "                    df = get_search_results(company)\n",
    "                    df.to_csv('scraped_data/KUNUNU_'+\"\".join(c for c in company if c.isalnum() or c in keepcharacters).rstrip()+\".csv\", \\\n",
    "                        index=False, encoding='utf-8', sep=';', quoting=csv.QUOTE_ALL)\n",
    "            except Exception as e:\n",
    "                errors_occurred = errors_occured + repr(e) + \"at company \" + company + \"\\n\"\n",
    "                if debug_prints:\n",
    "                    print('Some Error occured! Continue? y/y')\n",
    "                    print(e)\n",
    "                    test = input()\n",
    "                pass\n",
    "\n",
    "        print(\"Done!\")\n",
    "        if errors_occured != \"\":\n",
    "            print(\"The following errors occured:\")\n",
    "            print(errors_occured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get overview about companies from insolvency scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_insolvency_companies(print_title):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "\n",
    "    #insolvency_companies = pd.read_pickle('../Insolvency Scraper/Insolvency/companies_--+Alle+Bundesl_E4nder+--.pkl')\n",
    "    insolvency_companies = pd.read_pickle('../Insolvency Scraper/Insolvency/companies_Hessen.pkl')\n",
    "\n",
    "    debug_prints = False\n",
    "    errors_occured = \"\"\n",
    "    skip_item = False\n",
    "\n",
    "    # for company name sanitizing\n",
    "    keepcharacters = (' ','.','_', '-')\n",
    "    counter = 1\n",
    "\n",
    "\n",
    "    if 'insolvency_companies' in globals() or 'insolvency_companies' in locals():\n",
    "        for index, row in insolvency_companies.iterrows():\n",
    "            clear_output(wait=True)\n",
    "            print(print_title)\n",
    "            print(\"COMPANY:\", counter, \"/\", len(insolvency_companies.index), \"--\", row['Company Name'])\n",
    "            counter = counter+1\n",
    "\n",
    "            # do not check for names!\n",
    "            if len(row['Company Name'].split()) > 3:\n",
    "                skip_item = False\n",
    "            elif ('GmbH' in row['Company Name']) or ('GbR' in row['Company Name']) or \\\n",
    "                    ('AG' in row['Company Name']) or ('KG' in row['Company Name']) or ('OHG' in row['Company Name']) or \\\n",
    "                    ('SE' in row['Company Name']) or ('KG' in row['Company Name']):\n",
    "                skip_item = False\n",
    "            else:\n",
    "                skip_item = True\n",
    "                print('skipped!')\n",
    "\n",
    "            # get search results\n",
    "            if not skip_item:\n",
    "                try:\n",
    "                    df = get_search_results(row['Company Name'])\n",
    "                    df.to_csv('scraped_data/INSOLVENCY_'+\"\".join(c for c in row['Company Name'] if c.isalnum() or c in keepcharacters).rstrip()+\".csv\", \\\n",
    "                        index=False, encoding='utf-8', sep=';', quoting=csv.QUOTE_ALL)\n",
    "                except Exception as e:\n",
    "                    errors_occurred = errors_occured +  str(repr(e)) + \"\\n\"\n",
    "                    if debug_prints:\n",
    "                        print('Some Error occured! Continue? y/y')\n",
    "                        print(e)\n",
    "                        test = input()\n",
    "                    pass\n",
    "\n",
    "\n",
    "    print(\"Done!\")\n",
    "    if errors_occured != \"\":\n",
    "        print(\"The following errors occured:\")\n",
    "        print(errors_occured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get documents of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_documents_of_saved_csvs(from_year, print_title, save_captcha_files):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "\n",
    "    # get list of all files in folder 'scraped_data'\n",
    "    for root, dirs, files in os.walk('scraped_data'):\n",
    "        document_list = files\n",
    "        pass\n",
    "\n",
    "    abort_execution = False\n",
    "    skip_item = False\n",
    "\n",
    "    regex1 = re.compile(r'bis zum \\d{2}\\.\\d{2}\\.(\\d{4})')\n",
    "    current_filename = \"\"\n",
    "    if 'document_list' in globals() or 'document_list' in locals():\n",
    "        count_of_csvs = len([item for item in document_list if item.endswith('.csv')])\n",
    "    csv_counter = 1\n",
    "\n",
    "    # for company name sanitizing\n",
    "    keepcharacters = (' ','.','_', '-')\n",
    "\n",
    "    if 'document_list' in globals() or 'document_list' in locals():\n",
    "        for item in document_list:\n",
    "            clear_output(wait=True)\n",
    "            print(print_title)\n",
    "            print(\"COMPANY:\", csv_counter, \"/\", count_of_csvs, \"--\", item)\n",
    "            if item.endswith('.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv('scraped_data/'+item, sep=\";\")\n",
    "                    skip_item = False\n",
    "                except FileNotFoundError:\n",
    "                    print(\"file not found!\")\n",
    "                    skip_item = True\n",
    "            else:\n",
    "                skip_item = True\n",
    "\n",
    "            if not skip_item:\n",
    "                csv_counter = csv_counter + 1\n",
    "                for index, row in df.iterrows():\n",
    "                    if row[0] != \"Suche - kein Suchergebnis\" and isinstance(row[2], str):\n",
    "                        # filename like '%searchstring%_%documentdate%.html'\n",
    "                        current_filename = \"\".join(c for c in item if c.isalnum() or c in keepcharacters).rstrip()+\"_\"+row[3]+\".html\"\n",
    "                        # if the file for the current document does NOT exist, request it\n",
    "                        # else, the document will not be requested -- this avoids double work\n",
    "                        if not os.path.exists(os.path.join('scraped_data', current_filename)):\n",
    "                            t = regex1.search(row[2])\n",
    "                            if t is not None:\n",
    "                                if int(t.group()[-4:]) >= from_year:\n",
    "                                    print(\"next:\", row[0], \"-- Dokument vom\", row[3], \"--\", row[2])\n",
    "                                    if row[5] == session_id:\n",
    "                                        # here the document is requested:\n",
    "                                        html_result = get_document(row[6], row[7], save_captcha_files)\n",
    "                                        if html_result == \"exit\":\n",
    "                                            abort_execution = True\n",
    "                                            print(\"Execution was aborted!\")\n",
    "                                            break\n",
    "                                        elif html_result != \"\":\n",
    "                                            # filename like '%searchstring%_%documentdate%.html'\n",
    "                                            try:\n",
    "                                                with open(os.path.join('scraped_data', current_filename), \"w\", encoding='utf-8') as file:\n",
    "                                                    file.write(str(html_result))\n",
    "                                            except Exception as e:\n",
    "                                                print(\"could not save file\")\n",
    "                                    else:\n",
    "                                        print(\"Old searchresult, will be skipped.\")\n",
    "                        else:\n",
    "                            print(\"file already exists!\")\n",
    "                    else:\n",
    "                        break\n",
    "                if abort_execution:\n",
    "                    break\n",
    "\n",
    "\n",
    "        #   . for loop ends here\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling: Removing html documents with content \"invalid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_invalid_htmls(print_title):\n",
    "    global session_id\n",
    "    global debug_prints\n",
    "\n",
    "    # get list of all files in folder 'scraped_data'\n",
    "    for root, dirs, files in os.walk('scraped_data'):\n",
    "        document_list = files\n",
    "\n",
    "    abort_execution = False\n",
    "    skip_item = False\n",
    "\n",
    "    count_of_htmls = len([item for item in document_list if item.endswith('.html')])+1\n",
    "    html_counter = 1\n",
    "    deleted_counter = 0\n",
    "\n",
    "    for item in document_list:\n",
    "        clear_output(wait=True)\n",
    "        print(print_title)\n",
    "        print(\"Scanning document:\", html_counter, \"/\", count_of_htmls, \"--\", item)\n",
    "\n",
    "        if item.endswith('.html'):\n",
    "            try:\n",
    "                with open('scraped_data/'+item) as file:\n",
    "                    soup = BeautifulSoup(file)\n",
    "                skip_item = False\n",
    "            except Exception as EError:\n",
    "                print(\"An Error occured!\", repr(EError))\n",
    "                skip_item = True\n",
    "        else:\n",
    "            skip_item = True\n",
    "\n",
    "        if not skip_item:\n",
    "            html_counter = html_counter + 1\n",
    "            invalid_id = soup.findAll(class_='invalid')\n",
    "            if invalid_id:\n",
    "                print('this document is invalid. Will be deleted now')\n",
    "                try:\n",
    "                    if os.path.isfile('scraped_data/'+item):\n",
    "                        os.remove('scraped_data/'+item)\n",
    "                        deleted_counter = deleted_counter+1\n",
    "                except Exception as e:\n",
    "                    print(\"An Error occured!\", repr(e))\n",
    "                    pass\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(\"Deleted\", deleted_counter, \"html files because they had invalid content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ausführung:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Getting documents of saved CSVs ===\n",
      "COMPANY: 431 / 431 -- KUNUNU_Zoom7 GmbH.csv\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 1. Get Kununu Companies\n",
    "get_kununu_companies(540, 550, \"=== Getting KUNUNU companies ===\")\n",
    "time.sleep(2)\n",
    "\n",
    "# 2. Get Insolvency Companies\n",
    "# get_insolvency_companies(\"=== Getting INSOLVENCY companies ===\")\n",
    "\n",
    "# 3. Get Documents of saved CSVs\n",
    "\n",
    "get_documents_of_saved_csvs(2017, \"=== Getting documents of saved CSVs ===\", True)\n",
    "time.sleep(2)\n",
    "\n",
    "# 4. Remove invalid HTMLs\n",
    "# remove_invalid_htmls(\"=== Removing invalid HTMLS ===\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}