{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HTML Parser\n",
    "\n",
    "This Jupyter Notebook parses the HTML files in folder *scraped_data*."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# for html parsing:\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "# for file importing and exporting:\n",
    "import csv\n",
    "import pandas as pd\n",
    "# import json\n",
    "import os\n",
    "# other:\n",
    "import re # regex\n",
    "from IPython.display import clear_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "debug_prints = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def scan_htmls(print_title):\n",
    "    global debug_prints\n",
    "\n",
    "    debug_prints = True\n",
    "\n",
    "    # get list of all files in folder 'scraped_data'\n",
    "    for root, dirs, files in os.walk('scraped_data'):\n",
    "        document_list = files\n",
    "\n",
    "    # document_list = [\"KUNUNU_KH Software GmbH  Co.KG.csv_03.09.2019.html\"]\n",
    "    # document_list = ['KUNUNU_MathWorks.csv_28.02.2020.html', \"KUNUNU_KH Software GmbH  Co.KG.csv_03.09.2019.html\"]\n",
    "\n",
    "    abort_execution = False\n",
    "    skip_item = False\n",
    "\n",
    "    # for company name sanitizing\n",
    "    keepcharacters = (' ','.','_', '-')\n",
    "    employee_desc = ['Arbeitnehmer', 'Mitarbeiter']\n",
    "\n",
    "    count_of_htmls = len([item for item in document_list if item.endswith('.html')])\n",
    "    html_counter = 1\n",
    "    export_counter = 0\n",
    "    company_data = pd.DataFrame(columns=['Unternehmen','Dokumentendatum', 'Dokumententyp','JA_von', 'JA_bis', 'Anzahl_MA', 'Umsatzerlöse', 'Bilanz_Aktiva', 'Bilanz_Passiva', 'GuV', 'Dateiname'])\n",
    "    regexDates = re.compile(r'vom (\\d{2}\\.\\d{2}\\.\\d{4}) bis zum (\\d{2}\\.\\d{2}\\.\\d{4})')\n",
    "\n",
    "    for item in document_list:\n",
    "        clear_output(wait=True)\n",
    "        print(print_title)\n",
    "        print(\"Scanning document:\", html_counter, \"/\", count_of_htmls, \"--\", item)\n",
    "\n",
    "        temp_unternehmen = \"\"\n",
    "        temp_dokumententyp = \"\"\n",
    "        temp_JA_von = \"\"\n",
    "        temp_JA_bis = \"\"\n",
    "        temp_datum =\"\"\n",
    "        temp_anzahl_MA = \"\"\n",
    "        temp_umsatz = \"\"\n",
    "        temp_bilanzaktiva = \"\"\n",
    "        temp_bilanzpassiva = \"\"\n",
    "        temp_guv = \"\"\n",
    "        temp_dateiname = \"\"\n",
    "\n",
    "        if item.endswith('.html'):\n",
    "            try:\n",
    "                with open('scraped_data/'+item) as file:\n",
    "                    soup = BeautifulSoup(file)\n",
    "                skip_item = False\n",
    "            except Exception as EError:\n",
    "                print(\"An Error occured!\", repr(EError))\n",
    "                skip_item = True\n",
    "        else:\n",
    "            skip_item = True\n",
    "\n",
    "        if not skip_item:\n",
    "            html_counter = html_counter + 1\n",
    "\n",
    "            # do some magic here\n",
    "\n",
    "            temp_unternehmen = soup.find(\"h3\", class_=\"z_titel\").get_text(separator=\" \")\n",
    "            # for sanitizing the company name, sometimes there are tabs and shit\n",
    "            #temp_Unternehmen = \"\".join(c for c in temp_Unternehmen if c.isalnum() or c in keepcharacters).rstrip()\n",
    "            temp_unternehmen = \" \".join(temp_unternehmen.split())\n",
    "            temp_dokumententyp = soup.find(\"td\", class_=\"info\").get_text().split()[0]\n",
    "            temp_datum = soup.find(\"td\", class_=\"date\").get_text(separator=\" \")\n",
    "            temp_JA_von = regexDates.findall(soup.find(\"td\", class_=\"info\").get_text())[0][0]\n",
    "            temp_JA_bis = regexDates.findall(soup.find(\"td\", class_=\"info\").get_text())[0][1]\n",
    "            temp_dateiname = item\n",
    "\n",
    "\n",
    "            # find aktiva and passiva tables\n",
    "            start = soup.find(\"h3\", id=re.compile(\"^jp_Bilanz\", re.IGNORECASE))\n",
    "            temp_bilanz = []\n",
    "            if start:\n",
    "                for elem in start.next_siblings:\n",
    "                    if elem.name == 'h3':\n",
    "                        # print(\"found an h3:\", elem.text)\n",
    "                        break\n",
    "                    if elem.name != 'table':\n",
    "                        continue\n",
    "                    # it's a <table> tag before the next <h3>\n",
    "                    # print(\"table elem found\")\n",
    "                    temp_bilanz.append(elem)\n",
    "                    # print(\"Länge von tmep_bilnaz:\", len(temp_bilanz))\n",
    "            else:\n",
    "                print(\"no jp_Bilanz found\")\n",
    "\n",
    "            if len(temp_bilanz) > 1:\n",
    "                pass\n",
    "                # temp_bilanzaktiva = temp_bilanz[0]\n",
    "                # temp_bilanzpassiva = temp_bilanz[1]\n",
    "            if len(temp_bilanz) > 2: #there was a third table for GuV\n",
    "                print(\"GuV found!\")\n",
    "                temp_guv = temp_bilanz[2]\n",
    "\n",
    "            #find Umsatzerlöse\n",
    "            print(soup.find_all(string=re.compile('Umsatz', re.IGNORECASE)))\n",
    "            # if len(temp_bilanz) > 2: #there was a third table for GuV\n",
    "            #     for elem in temp_guv.find('td', string=re.compile('Umsatz', re.IGNORECASE)):\n",
    "            #             print(\"Umsatz found:\", elem)\n",
    "\n",
    "\n",
    "            # find count of employees\n",
    "            ma_sentence_found = False\n",
    "            search_element = soup.find_all('p')\n",
    "            for elem in search_element:\n",
    "                # if elem.text contains a string out of employee_desc\n",
    "                if any(x in elem.text for x in employee_desc):\n",
    "                    # if there are any br tag children\n",
    "                    # https://stackoverflow.com/questions/5275359/using-beautifulsoup-to-extract-text-between-line-breaks-e-g-br-tags\n",
    "                    for br in elem.find_all('br'):\n",
    "                        next_s = br.next_sibling\n",
    "                        if not (next_s and isinstance(next_s,NavigableString)):\n",
    "                            continue\n",
    "                        next2_s = next_s.next_sibling\n",
    "                        if next2_s and isinstance(next2_s,Tag) and next2_s.name == 'br':\n",
    "                            text = str(next_s)\n",
    "                            if any(x in text for x in employee_desc):\n",
    "                                if debug_prints:\n",
    "                                    print(\"MA found ins brs:\", \" \".join(text.split()))\n",
    "                                    print(\"MA found ins brs:\", \" \".join(text.split()))\n",
    "                                temp_anzahl_MA = \" \".join(text.split())\n",
    "                                ma_sentence_found = True\n",
    "                                break\n",
    "                    if not ma_sentence_found:\n",
    "                        if debug_prints:\n",
    "                            print(\"MA not found in brs, but directly in p tag:\", \" \".join(elem.text.split()))\n",
    "                        temp_anzahl_MA = \" \".join(elem.text.split())\n",
    "\n",
    "                    break\n",
    "\n",
    "            print(temp_anzahl_MA)\n",
    "\n",
    "            test = [temp_unternehmen,temp_datum,temp_dokumententyp,temp_JA_von,temp_JA_bis,\n",
    "                    temp_anzahl_MA,temp_umsatz,temp_bilanzaktiva,temp_bilanzpassiva,temp_guv,temp_dateiname]\n",
    "            # print(test)\n",
    "            #company_data.append(test, ignore_index=True)\n",
    "            company_data.loc[len(company_data)] = test\n",
    "            export_counter = export_counter + 1\n",
    "            # print(company_data.describe())\n",
    "            #print(company_data)\n",
    "            #company_data.to_pickle('output/company_attributes.pkl')\n",
    "            # print(company_data.loc[0,'Anzahl_MA'])\n",
    "\n",
    "\n",
    "            # print(\"Unternehmen:\", temp_unternehmen)\n",
    "            # print(\"Dokumentendatum:\", temp_datum)\n",
    "            # print(\"Dokumententyp:\", temp_dokumententyp)\n",
    "            # print(\"Daten von-bis:\", temp_JA_von, temp_JA_bis)\n",
    "            # print(\"Anzahl MA:\", temp_anzahl_MA)\n",
    "            # print(\"Umsatzerlöse:\", temp_umsatz)\n",
    "            # print(\"Bilanz Aktiva Länge:\", len(temp_bilanzaktiva))\n",
    "            # print(\"Bilanz Passiva Länge:\", len(temp_bilanzpassiva))\n",
    "            # print(\"GuV Länge:\", len(temp_guv))\n",
    "            # print(\"Dateiname:\", temp_dateiname)\n",
    "\n",
    "    company_data.to_csv('output/company_attributes.csv', index=False, encoding='utf-8', sep=';', quoting=csv.QUOTE_ALL)\n",
    "    print(\"Done!\")\n",
    "    print(\"Found and exported values out of\", export_counter, \"html files.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "Scanning document: 226 / 225 -- KUNUNU_Zoom7 GmbH.csv\n",
      "Done!\n",
      "Found and exported values out of 225 html files.\n",
      "Länge\n"
     ]
    }
   ],
   "source": [
    "scan_htmls(\"title\")\n",
    "print(\"Länge\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Struktur\n",
    "* Name des Unternehmens\n",
    "* Typ des Abschlusses\n",
    "* Jahresabschluss_von\n",
    "* Jahresabschluss_bis\n",
    "* Datum der Veröffentlichung\n",
    "* Anzahl der MA\n",
    "* (optional:) Umsatzerlöse\n",
    "* Bilanz Aktiva\n",
    "* Bilanz Passiva\n",
    "* (optional:) GuV\n",
    "* Dateiname\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}